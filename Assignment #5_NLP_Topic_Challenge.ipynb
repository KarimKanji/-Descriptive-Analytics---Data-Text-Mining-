{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM6tDbReUIBDB+c5/MerFAN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#NLP Topic Challenge"],"metadata":{"id":"xg_cC7lo-sdH"}},{"cell_type":"markdown","source":["### Imports and downloads"],"metadata":{"id":"A7Aj9z3m-yo0"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRtIAERe7ABQ","executionInfo":{"status":"ok","timestamp":1677487120716,"user_tz":-120,"elapsed":3819,"user":{"displayName":"Karim Kanji","userId":"05089157546900411619"}},"outputId":"cfc85dc9-7cb1-4a09-ad5f-f2c0012f0ec2"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["import gensim\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from collections.abc import Iterable\n","from gensim import corpora\n","from gensim.models import LsiModel\n","from gensim.models.coherencemodel import CoherenceModel\n","import matplotlib.pyplot as plt\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from gensim import corpora, models\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Tokenization, lemmatization and creating dictionary and corpus\n"],"metadata":{"id":"_Cq3rsYG-1pX"}},{"cell_type":"code","source":["# paragraph = \"\"\"The new restaurant in town has become quite popular lately.\\n\n","# People love their food and the ambiance. The restaurant has a beautiful view \\n\n","# of the city and the prices are reasonable. The staff is friendly and attentive.\\n\n","# They offer a wide variety of dishes, including vegan and gluten-free options. Overall, it's a great place to dine with friends and family.\"\"\"\n","\n","paragraph = \"\"\"People often tell me that Finland is the happiest country in the world. \\n\n","In fact, most statisticians agree that Finland reports to the happiest country.\\n\n","But, is it really so easy to explain?\\n\n","I mean, the public facility are excellent.\\n\n","Safety is probably best in the world.\\n\n","Of course, the weather is not so great.\\n\n","And there is a less than ideal neighbor to the East.\\n\n","But then, anywhere in Nordics, we have the same safety, the same facilities and the same weather.\\n\n","Most of Western Europe provides you comparable facilities.\\n\n","As a relatively new Finn, I have a different perspective on the happiness of Finns.\\n\n","I believe that Finns automatically expect a bad weather, no help from neighbors and in general tough conditions.\\n\n","And this low expectation leads to the fact that Finns are almost always pleasantly surprised by the actual observations.\\n\n","So, the inherently low expectations of Finns create ideal situations for their expectations to be substantially surpassed.\\n\n","This leads to a perception that all is well in the world.\\n\n","When in reality, all that happens is that Finns' reality matches or surpasses their low expectations.\\n\n","If there is a lesson here, it is not in the social order, the public facilities or even in safety.\\n\n","The true lesson is in setting our expectations.\\n\n","If we demand less from less, we expect less from the Provider, then life can easily ensure our happiness.\"\"\"\n","\n","\n","sentences = nltk.sent_tokenize(paragraph)\n","tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","processed_sentences = []\n","for sentence in tokenized_sentences:\n","    # Remove stop words and punctuation\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token.lower() for token in sentence if token.lower() not in stop_words and token.isalpha()]\n","\n","    # Lemmatize the tokens\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    \n","    processed_sentences.append(tokens)\n","    \n","# Create a dictionary and corpus\n","dictionary = corpora.Dictionary(processed_sentences)\n","corpus = [dictionary.doc2bow(sentence) for sentence in processed_sentences]"],"metadata":{"id":"JAnfQRbn8rdX","executionInfo":{"status":"ok","timestamp":1677487124607,"user_tz":-120,"elapsed":3896,"user":{"displayName":"Karim Kanji","userId":"05089157546900411619"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Topics "],"metadata":{"id":"9XyTtRZ5-_--"}},{"cell_type":"code","source":["# Train a topic model using LDA\n","lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=50, passes=100)\n","\n","# Print the topics\n","for topic in lda.print_topics():\n","    print(topic)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xvmqzgg8uzP","executionInfo":{"status":"ok","timestamp":1677487680876,"user_tz":-120,"elapsed":817,"user":{"displayName":"Karim Kanji","userId":"05089157546900411619"}},"outputId":"11c500e9-13da-4959-dbe4-02d52080633e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(38, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(2, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(36, '0.223*\"easy\" + 0.223*\"really\" + 0.223*\"explain\" + 0.004*\"lead\" + 0.004*\"inherently\" + 0.004*\"create\" + 0.004*\"surprised\" + 0.004*\"pleasantly\" + 0.004*\"observation\" + 0.004*\"low\"')\n","(6, '0.088*\"world\" + 0.088*\"tell\" + 0.088*\"finland\" + 0.088*\"country\" + 0.088*\"people\" + 0.088*\"happiest\" + 0.088*\"often\" + 0.088*\"course\" + 0.088*\"weather\" + 0.088*\"great\"')\n","(10, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(12, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(33, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(32, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(25, '0.235*\"finn\" + 0.119*\"happiness\" + 0.119*\"perspective\" + 0.119*\"new\" + 0.119*\"relatively\" + 0.119*\"different\" + 0.002*\"observation\" + 0.002*\"low\" + 0.002*\"expectation\" + 0.002*\"surprised\"')\n","(21, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(7, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(13, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(16, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(3, '0.155*\"weather\" + 0.155*\"facility\" + 0.155*\"anywhere\" + 0.155*\"nordic\" + 0.155*\"safety\" + 0.003*\"observation\" + 0.003*\"inherently\" + 0.003*\"create\" + 0.003*\"surprised\" + 0.003*\"pleasantly\"')\n","(46, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(14, '0.174*\"expectation\" + 0.088*\"substantially\" + 0.088*\"situation\" + 0.088*\"create\" + 0.088*\"surpassed\" + 0.088*\"ideal\" + 0.088*\"low\" + 0.088*\"inherently\" + 0.088*\"finn\" + 0.002*\"surprised\"')\n","(45, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(48, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(47, '0.013*\"always\" + 0.013*\"almost\" + 0.013*\"create\" + 0.013*\"surprised\" + 0.013*\"pleasantly\" + 0.013*\"observation\" + 0.013*\"low\" + 0.013*\"lead\" + 0.013*\"expectation\" + 0.013*\"situation\"')\n","(41, '0.183*\"east\" + 0.183*\"ideal\" + 0.183*\"le\" + 0.183*\"neighbor\" + 0.004*\"low\" + 0.004*\"expectation\" + 0.004*\"surprised\" + 0.004*\"pleasantly\" + 0.004*\"observation\" + 0.004*\"inherently\"')\n"]}]},{"cell_type":"markdown","source":["#Assignment:"],"metadata":{"id":"_5aQWbTwBXPt"}},{"cell_type":"markdown","source":["### Now that we have the topics we can make a function that takes in question X, Y and Z and returns an anwser on wether or not the requested topics match the topics of the document"],"metadata":{"id":"NysEVz8sBaDv"}},{"cell_type":"code","source":["# Add your questions here\n","X, Y, Z = \"\",\"\",\"\"\n","\n","X= \"finn\"\n","Y= \"automatically\"\n","Z= \"easilautomaticallyydddddd\"\n","\n","# Define the function\n","def anwser(lda, X, Y, Z):\n","    x_found = False\n","    y_found = False\n","    z_found = False\n","    \n","# Loop through the lda topics\n","    for _, topic_string in lda.print_topics():\n","\n","# Split the result so it only contains the words we are looking for\n","        topic_words = [word.split('*')[1].strip()[1:-1] for word in topic_string.split('+')]\n","        if X in topic_words:\n","            x_found = True\n","        if Y in topic_words:\n","            y_found = True\n","        if Z in topic_words:\n","            z_found = True\n","            \n","    print(f\"Does any topic contain the string '{X}'? {'Yes' if x_found else 'No'}\")\n","    print(f\"Does any topic contain the string '{Y}'? {'Yes' if y_found else 'No'}\")\n","    print(f\"Does any topic contain the string '{Z}'? {'Yes' if z_found else 'No'}\")\n","\n","\n","anwser(lda, X, Y, Z)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XVay0VtBsxg","executionInfo":{"status":"ok","timestamp":1677487646139,"user_tz":-120,"elapsed":216,"user":{"displayName":"Karim Kanji","userId":"05089157546900411619"}},"outputId":"baf33b06-08e3-4c6d-82c4-8e84bb5a4381"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Does any topic contain the string 'finn'? Yes\n","Does any topic contain the string 'automatically'? No\n","Does any topic contain the string 'easilautomaticallyydddddd'? No\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nk-0zrSpDiLW"},"execution_count":null,"outputs":[]}]}